# æš‘å‡å­¦ä¹ è®°å½•
## 8.13
### æ­£äº¤åŒ–
æ­£äº¤åŒ–çš„æ¦‚å¿µæ˜¯æŒ‡ï¼Œä½ å¯ä»¥æƒ³å‡ºä¸€ä¸ªç»´åº¦ï¼Œè¿™ä¸ªç»´åº¦ä½ æƒ³åšçš„æ˜¯æ§åˆ¶è½¬å‘è§’ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªç»´åº¦æ¥æ§åˆ¶ä½ çš„é€Ÿåº¦ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦ä¸€ä¸ªæ—‹é’®å°½é‡åªæ§åˆ¶è½¬å‘è§’ï¼Œå¦ä¸€ä¸ªæ—‹é’®ï¼Œåœ¨è¿™ä¸ªå¼€è½¦çš„ä¾‹å­é‡Œå…¶å®æ˜¯æ²¹é—¨å’Œåˆ¹è½¦æ§åˆ¶äº†ä½ çš„é€Ÿåº¦ã€‚ä½†å¦‚æœä½ æœ‰ä¸€ä¸ªæ§åˆ¶æ—‹é’®å°†ä¸¤è€…æ··åœ¨ä¸€èµ·ï¼Œæ¯”å¦‚è¯´è¿™æ ·ä¸€ä¸ªæ§åˆ¶è£…ç½®åŒæ—¶å½±å“ä½ çš„è½¬å‘è§’å’Œé€Ÿåº¦ï¼ŒåŒæ—¶æ”¹å˜äº†ä¸¤ä¸ªæ€§è´¨ï¼Œé‚£ä¹ˆå°±å¾ˆéš¾ä»¤ä½ çš„è½¦å­ä»¥æƒ³è¦çš„é€Ÿåº¦å’Œè§’åº¦å‰è¿›ã€‚ç„¶è€Œæ­£äº¤åŒ–ä¹‹åï¼Œæ­£äº¤æ„å‘³ç€äº’æˆ 90 åº¦ã€‚è®¾è®¡å‡ºæ­£äº¤åŒ–çš„æ§åˆ¶è£…ç½®ï¼Œæœ€ç†æƒ³çš„æƒ…å†µæ˜¯å’Œä½ å®é™…æƒ³æ§åˆ¶çš„æ€§è´¨ä¸€è‡´ï¼Œè¿™æ ·ä½ è°ƒæ•´å‚æ•°æ—¶å°±å®¹æ˜“å¾—å¤šã€‚å¯ä»¥å•ç‹¬è°ƒæ•´è½¬å‘è§’ï¼Œè¿˜æœ‰ä½ çš„æ²¹é—¨å’Œåˆ¹è½¦ï¼Œä»¤è½¦å­ä»¥ä½ æƒ³è¦çš„æ–¹å¼åŠ¨ã€‚     
åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¦‚æœä½ å¯ä»¥è§‚å¯Ÿä½ çš„ç³»ç»Ÿï¼Œç„¶åè¯´è¿™ä¸€éƒ¨åˆ†æ˜¯é”™çš„ï¼Œå®ƒåœ¨è®­ç»ƒé›†ä¸Šåšçš„ä¸å¥½ã€åœ¨å¼€å‘é›†ä¸Šåšçš„ä¸å¥½ã€å®ƒåœ¨æµ‹è¯•é›†ä¸Šåšçš„ä¸å¥½ï¼Œæˆ–è€…å®ƒåœ¨æµ‹è¯•é›†ä¸Šåšçš„ä¸é”™ï¼Œä½†åœ¨ç°å®ä¸–ç•Œä¸­ä¸å¥½ï¼Œè¿™å°±å¾ˆå¥½ã€‚å¿…é¡»å¼„æ¸…æ¥šåˆ°åº•æ˜¯ä»€ä¹ˆåœ°æ–¹å‡ºé—®é¢˜äº†ï¼Œç„¶åæˆ‘ä»¬åˆšå¥½æœ‰å¯¹åº”çš„æ—‹é’®ï¼Œæˆ–è€…ä¸€ç»„å¯¹åº”çš„æ—‹é’®ï¼Œåˆšå¥½å¯ä»¥è§£å†³é‚£ä¸ªé—®é¢˜ï¼Œé‚£ä¸ªé™åˆ¶äº†æœºå™¨å­¦ä¹ ç³»ç»Ÿæ€§èƒ½çš„é—®é¢˜ã€‚  
### å•ä¸€æ•°å­—è¯„ä¼°
æ— è®ºä½ æ˜¯è°ƒæ•´è¶…å‚æ•°ï¼Œæˆ–è€…æ˜¯å°è¯•ä¸åŒçš„å­¦ä¹ ç®—æ³•ï¼Œæˆ–è€…åœ¨æ­å»ºæœºå™¨å­¦ä¹ ç³»ç»Ÿæ—¶å°è¯•ä¸åŒæ‰‹æ®µï¼Œä½ ä¼šå‘ç°ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªå•å®æ•°è¯„ä¼°æŒ‡æ ‡ï¼Œä½ çš„è¿›å±•ä¼šå¿«å¾—å¤šï¼Œå®ƒå¯ä»¥å¿«é€Ÿå‘Šè¯‰ä½ ï¼Œæ–°å°è¯•çš„æ‰‹æ®µæ¯”ä¹‹å‰çš„æ‰‹æ®µå¥½è¿˜æ˜¯å·®ã€‚  
å¦‚æœä½ è¦è€ƒè™‘ğ‘ä¸ªæŒ‡æ ‡ï¼Œæœ‰æ—¶å€™é€‰æ‹©å…¶ä¸­ä¸€ä¸ªæŒ‡æ ‡åšä¸ºä¼˜åŒ–æŒ‡æ ‡æ˜¯åˆç†çš„ã€‚æ‰€ä»¥ä½ æƒ³å°½é‡ä¼˜åŒ–é‚£ä¸ªæŒ‡æ ‡ï¼Œç„¶åå‰©ä¸‹ğ‘ âˆ’ 1ä¸ªæŒ‡æ ‡éƒ½æ˜¯æ»¡è¶³æŒ‡æ ‡ï¼Œæ„å‘³ç€åªè¦å®ƒä»¬è¾¾åˆ°ä¸€å®šé˜ˆå€¼ï¼Œä¾‹å¦‚è¿è¡Œæ—¶é—´å¿«äº 100 æ¯«ç§’ï¼Œä½†åªè¦è¾¾åˆ°ä¸€å®šçš„é˜ˆå€¼ï¼Œä½ ä¸åœ¨ä¹å®ƒè¶…è¿‡é‚£ä¸ªé—¨æ§›ä¹‹åçš„è¡¨ç°ï¼Œä½†å®ƒä»¬å¿…é¡»è¾¾åˆ°è¿™ä¸ªé—¨æ§›ã€‚  
### è®­ç»ƒ/å¼€å‘/æµ‹è¯•é›†åˆ’åˆ†
æœºå™¨å­¦ä¹ ä¸­çš„å·¥ä½œæµç¨‹æ˜¯ï¼Œä½ å°è¯•å¾ˆå¤šæ€è·¯ï¼Œç”¨è®­ç»ƒé›†è®­ç»ƒä¸åŒçš„æ¨¡å‹ï¼Œç„¶åä½¿ç”¨å¼€å‘é›†æ¥è¯„ä¼°ä¸åŒçš„æ€è·¯ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªï¼Œç„¶åä¸æ–­è¿­ä»£å»æ”¹å–„å¼€å‘é›†çš„æ€§èƒ½ï¼Œç›´åˆ°æœ€åä½ å¯ä»¥å¾—åˆ°ä¸€ä¸ªä»¤ä½ æ»¡æ„çš„æˆæœ¬ï¼Œç„¶åä½ å†ç”¨æµ‹è¯•é›†å»è¯„ä¼°ã€‚  
å°†æ‰€æœ‰æ•°æ®éšæœºæ´—ç‰Œï¼Œæ”¾å…¥å¼€å‘é›†å’Œæµ‹è¯•é›†ï¼Œå¹¶ä¸”å¼€å‘é›†å’Œæµ‹è¯•é›†éƒ½æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œè¿™åˆ†å¸ƒå°±æ˜¯ä½ çš„æ‰€æœ‰æ•°æ®æ··åœ¨ä¸€èµ·ã€‚
### å¯é¿å…åå·®
å¦‚æœè´å¶æ–¯é”™è¯¯ç‡æ˜¯ 7.5%ã€‚ä½ å®é™…ä¸Šå¹¶ä¸æƒ³å¾—åˆ°ä½äºè¯¥çº§åˆ«çš„é”™è¯¯ç‡ï¼Œæ‰€ä»¥ä½ ä¸ä¼šè¯´ä½ çš„è®­ç»ƒé”™è¯¯ç‡æ˜¯ 8%ï¼Œç„¶å 8%å°±è¡¡é‡äº†ä¾‹å­ä¸­çš„åå·®å¤§å°ã€‚ä½ åº”è¯¥è¯´ï¼Œå¯é¿å…åå·®å¯èƒ½åœ¨ 0.5%å·¦å³ï¼Œæˆ–è€… 0.5%æ˜¯å¯é¿å…åå·®çš„æŒ‡æ ‡ã€‚è€Œè¿™ä¸ª 2%æ˜¯æ–¹å·®çš„æŒ‡æ ‡ï¼Œæ‰€ä»¥è¦å‡å°‘è¿™ä¸ª 2%æ¯”å‡å°‘è¿™ä¸ª 0.5%ç©ºé—´è¦å¤§å¾—å¤šã€‚è€Œåœ¨å·¦è¾¹çš„ä¾‹å­ä¸­ï¼Œè¿™ 7%è¡¡é‡äº†å¯é¿å…åå·®å¤§å°ï¼Œè€Œ 2%è¡¡é‡äº†æ–¹å·®å¤§å°ã€‚æ‰€ä»¥åœ¨å·¦è¾¹è¿™ä¸ªä¾‹å­é‡Œï¼Œä¸“æ³¨å‡å°‘å¯é¿å…åå·®å¯èƒ½æ½œåŠ›æ›´å¤§ã€‚  
## 8.14(å·ç§¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬çŸ¥è¯†)
å·»ç§¯ç¥ç»ç½‘ç»œå°†æ·±åº¦å­¦ä¹ åº”ç”¨åˆ°è®¡ç®—æœºè§†è§‰ä¸­ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ç®—æ³•ä¸­çš„ä¸€ç§ï¼Œå…¶éšå«å±‚å†…çš„å·ç§¯æ ¸å‚æ•°å…±äº«å’Œå±‚é—´è¿æ¥çš„ç¨€ç–æ€§ä½¿å¾—å·ç§¯ç¥ç»ç½‘ç»œèƒ½å¤Ÿä»¥è¾ƒå°çš„è®¡ç®—é‡å¯¹æ ¼ç‚¹åŒ–ç‰¹å¾ï¼Œå‡å°‘äº†å‚æ•°ï¼ŒåŠ å¿«äº†é€Ÿåº¦ã€‚
å·ç§¯ç¥ç»ç½‘ç»œçš„éšå«å±‚åŒ…å«å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚ã€‚  
### å·ç§¯å±‚
å·»ç§¯å±‚æœ€ä¸»è¦çš„å°±æ˜¯è¿›è¡Œå·»ç§¯è¿ç®—ï¼Œå…¶å®å°±æ˜¯cvä¸­çš„å·»ç§¯æ ¸ï¼Œåªä¸è¿‡å·»ç§¯æ ¸çš„å‚æ•°éƒ½æ˜¯éšæœºçš„ï¼Œåé€šè¿‡åå‘ä¼ æ’­ä¸€æ¬¡æ¬¡æ›´æ–°ï¼Œåªåˆ°æœ€ä½³æ•ˆæœã€‚  
### æ± åŒ–å±‚
æ± åŒ–å±‚åˆ†ä¸ºæœ€å¤§æ± åŒ–å±‚å’Œå‡å€¼æ± åŒ–å±‚ã€‚æœ€å¤§æ± åŒ–å±‚å…¶å®å°±æ˜¯å½“å‰ä½ç½®å–å·»ç§¯æ ¸èŒƒå›´å†…æœ€å¤§çš„æ•°æ¥ä»£æ›¿ï¼Œè€Œå‡å€¼æ± åŒ–å±‚åˆ™æ˜¯ç”¨å‡å€¼å¡«å…¥ã€‚ï¼ˆè¿™é‡Œå¹¶æ²¡æœ‰å¾ˆæ‡‚æœ‰ä»€ä¹ˆç”¨ï¼Œä¸è¿‡å®éªŒè¯æ˜ï¼Œå¯¹å·»ç§¯ç¥ç»ç½‘ç»œæ•ˆæœå¾ˆå¥½ã€‚ï¼‰  
### å…¨è¿æ¥å±‚
å…¶å®å°±æ˜¯æˆ‘ä»¬ä¹‹å‰å†™çš„åŸºç¡€ç¥ç»ç½‘ç»œå±‚ï¼Œä¸€èˆ¬éƒ½æ˜¯å°†æ± åŒ–å±‚çš„ç»“æœè¿›è¡Œä¸€ç»´åŒ–ï¼Œè¿™æ ·ç›¸æ¯”ç›´æ¥ç”¨å…¨è¿æ¥å±‚ï¼Œä¼šå°‘äº†å¾ˆå¤šå¾ˆå¤šæ•°æ®ï¼Œå¤§å¤§æé«˜äº†æ•°åº¦ã€‚

### ä»£ç ç¤ºèŒƒ
#### åˆ›å»ºplaceholder
```
def create_placeholders(n_H0, n_W0, n_C0, n_y):
    """
    ä¸ºsessionåˆ›å»ºå ä½ç¬¦
    
    å‚æ•°ï¼š
        n_H0 - å®æ•°ï¼Œè¾“å…¥å›¾åƒçš„é«˜åº¦
        n_W0 - å®æ•°ï¼Œè¾“å…¥å›¾åƒçš„å®½åº¦
        n_C0 - å®æ•°ï¼Œè¾“å…¥çš„é€šé“æ•°
        n_y  - å®æ•°ï¼Œåˆ†ç±»æ•°
        
    è¾“å‡ºï¼š
        X - è¾“å…¥æ•°æ®çš„å ä½ç¬¦ï¼Œç»´åº¦ä¸º[None, n_H0, n_W0, n_C0]ï¼Œç±»å‹ä¸º"float"
        Y - è¾“å…¥æ•°æ®çš„æ ‡ç­¾çš„å ä½ç¬¦ï¼Œç»´åº¦ä¸º[None, n_y]ï¼Œç»´åº¦ä¸º"float"
    """
    X = tf.placeholder(tf.float32,[None, n_H0, n_W0, n_C0])
    Y = tf.placeholder(tf.float32,[None, n_y])
    
    return X,Y

```
#### åˆå§‹åŒ–å‚æ•°
```
def initialize_parameters():
    """
    åˆå§‹åŒ–æƒå€¼çŸ©é˜µï¼Œè¿™é‡Œæˆ‘ä»¬æŠŠæƒå€¼çŸ©é˜µç¡¬ç¼–ç ï¼š
    W1 : [4, 4, 3, 8]
    W2 : [2, 2, 8, 16]
    
    è¿”å›ï¼š
        åŒ…å«äº†tensorç±»å‹çš„W1ã€W2çš„å­—å…¸
    """
    tf.set_random_seed(1)
    
    W1 = tf.get_variable("W1",[4,4,3,8],initializer=tf.contrib.layers.xavier_initializer(seed=0))
    W2 = tf.get_variable("W2",[2,2,8,16],initializer=tf.contrib.layers.xavier_initializer(seed=0))
    
    parameters = {"W1": W1,
                  "W2": W2}
    
    return parameters

```
#### å‰å‘ä¼ æ’­
åœ¨TensorFlowé‡Œé¢æœ‰ä¸€äº›å¯ä»¥ç›´æ¥æ‹¿æ¥ç”¨çš„å‡½æ•°ï¼š  
tf.nn.conv2d(X,W1,strides=[1,s,s,1],padding='SAME') ç»™å®šè¾“å…¥ X X Xå’Œä¸€ç»„è¿‡æ»¤å™¨ W 1 W1 W1ï¼Œè¿™ä¸ªå‡½æ•°å°†ä¼šè‡ªåŠ¨ä½¿ç”¨ W 1 W1 W1æ¥å¯¹ X X Xè¿›è¡Œå·ç§¯ï¼Œç¬¬ä¸‰ä¸ªè¾“å…¥å‚æ•°æ˜¯**[1,s,s,1]**æ˜¯æŒ‡å¯¹äºè¾“å…¥ (m, n_H_prev, n_W_prev, n_C_prev)è€Œè¨€ï¼Œæ¯æ¬¡æ»‘åŠ¨çš„æ­¥ä¼ã€‚  
tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME')ï¼šç»™å®šè¾“å…¥ X X Xï¼Œè¯¥å‡½æ•°å°†ä¼šä½¿ç”¨å¤§å°ä¸ºï¼ˆf,fï¼‰ä»¥åŠæ­¥ä¼ä¸º(s,s)çš„çª—å£å¯¹å…¶è¿›è¡Œæ»‘åŠ¨å–æœ€å¤§å€¼ã€‚  
tf.nn.relu(Z1)ï¼šè®¡ç®—Z1çš„ReLUæ¿€æ´»ã€‚  
tf.contrib.layers.flatten(P)ï¼šç»™å®šä¸€ä¸ªè¾“å…¥Pï¼Œæ­¤å‡½æ•°å°†ä¼šæŠŠæ¯ä¸ªæ ·æœ¬è½¬åŒ–æˆä¸€ç»´çš„å‘é‡ï¼Œç„¶åè¿”å›ä¸€ä¸ªtensorå˜é‡ï¼Œå…¶ç»´åº¦ä¸ºï¼ˆbatch_size,kï¼‰  
tf.contrib.layers.fully_connected(F, num_outputs)ï¼šç»™å®šä¸€ä¸ªå·²ç»ä¸€ç»´åŒ–äº†çš„è¾“å…¥Fï¼Œæ­¤å‡½æ•°å°†ä¼šè¿”å›ä¸€ä¸ªç”±å…¨è¿æ¥å±‚è®¡ç®—è¿‡åçš„è¾“å‡ºï¼Œä½¿ç”¨tf.contrib.layers.fully_connected(F, num_outputs)çš„æ—¶å€™ï¼Œå…¨è¿æ¥å±‚ä¼šè‡ªåŠ¨åˆå§‹åŒ–æƒå€¼ä¸”åœ¨ä½ è®­ç»ƒæ¨¡å‹çš„æ—¶å€™å®ƒä¹Ÿä¼šä¸€ç›´å‚ä¸ï¼Œæ‰€ä»¥å½“æˆ‘ä»¬åˆå§‹åŒ–å‚æ•°çš„æ—¶å€™æˆ‘ä»¬ä¸éœ€è¦ä¸“é—¨å»åˆå§‹åŒ–å®ƒçš„æƒå€¼ã€‚  
```
def forward_propagation(X,parameters):
    """
    å®ç°å‰å‘ä¼ æ’­
    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED
    
    å‚æ•°ï¼š
        X - è¾“å…¥æ•°æ®çš„placeholderï¼Œç»´åº¦ä¸º(è¾“å…¥èŠ‚ç‚¹æ•°é‡ï¼Œæ ·æœ¬æ•°é‡)
        parameters - åŒ…å«äº†â€œW1â€å’Œâ€œW2â€çš„pythonå­—å…¸ã€‚
        
    è¿”å›ï¼š
        Z3 - æœ€åä¸€ä¸ªLINEARèŠ‚ç‚¹çš„è¾“å‡º
    
    """
    W1 = parameters['W1']
    W2 = parameters['W2']
    
    #Conv2d : æ­¥ä¼ï¼š1ï¼Œå¡«å……æ–¹å¼ï¼šâ€œSAMEâ€
    Z1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding="SAME")
    #ReLU ï¼š
    A1 = tf.nn.relu(Z1)
    #Max pool : çª—å£å¤§å°ï¼š8x8ï¼Œæ­¥ä¼ï¼š8x8ï¼Œå¡«å……æ–¹å¼ï¼šâ€œSAMEâ€
    P1 = tf.nn.max_pool(A1,ksize=[1,8,8,1],strides=[1,8,8,1],padding="SAME")
    
    #Conv2d : æ­¥ä¼ï¼š1ï¼Œå¡«å……æ–¹å¼ï¼šâ€œSAMEâ€
    Z2 = tf.nn.conv2d(P1,W2,strides=[1,1,1,1],padding="SAME")
    #ReLU ï¼š
    A2 = tf.nn.relu(Z2)
    #Max pool : è¿‡æ»¤å™¨å¤§å°ï¼š4x4ï¼Œæ­¥ä¼ï¼š4x4ï¼Œå¡«å……æ–¹å¼ï¼šâ€œSAMEâ€
    P2 = tf.nn.max_pool(A2,ksize=[1,4,4,1],strides=[1,4,4,1],padding="SAME")
    
    #ä¸€ç»´åŒ–ä¸Šä¸€å±‚çš„è¾“å‡º
    P = tf.contrib.layers.flatten(P2)
    
    #å…¨è¿æ¥å±‚ï¼ˆFCï¼‰ï¼šä½¿ç”¨æ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥å±‚
    Z3 = tf.contrib.layers.fully_connected(P,6,activation_fn=None)
    
    return Z3
    

```
#### è®¡ç®—æˆæœ¬
```
def compute_cost(Z3,Y):
    """
    è®¡ç®—æˆæœ¬
    å‚æ•°ï¼š
        Z3 - æ­£å‘ä¼ æ’­æœ€åä¸€ä¸ªLINEARèŠ‚ç‚¹çš„è¾“å‡ºï¼Œç»´åº¦ä¸ºï¼ˆ6ï¼Œæ ·æœ¬æ•°ï¼‰ã€‚
        Y - æ ‡ç­¾å‘é‡çš„placeholderï¼Œå’ŒZ3çš„ç»´åº¦ç›¸åŒ
    
    è¿”å›ï¼š
        cost - è®¡ç®—åçš„æˆæœ¬
    
    """
    
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))
    
    return cost

```
#### æ„å»ºæ¨¡å‹
åœ¨å®ç°è¿™ä¸ªæ¨¡å‹çš„æ—¶å€™æˆ‘ä»¬è¦ç»å†ä»¥ä¸‹æ­¥éª¤ï¼š  
1. åˆ›å»ºå ä½ç¬¦
2. åˆå§‹åŒ–å‚æ•°
3. å‰å‘ä¼ æ’­
4. è®¡ç®—æˆæœ¬
5. åå‘ä¼ æ’­
6. åˆ›å»ºä¼˜åŒ–å™¨
```
def model(X_train, Y_train, X_test, Y_test, learning_rate=0.009, 
         num_epochs=100,minibatch_size=64,print_cost=True,isPlot=True):
    """
    ä½¿ç”¨TensorFlowå®ç°ä¸‰å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œ
    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED
    
    å‚æ•°ï¼š
        X_train - è®­ç»ƒæ•°æ®ï¼Œç»´åº¦ä¸º(None, 64, 64, 3)
        Y_train - è®­ç»ƒæ•°æ®å¯¹åº”çš„æ ‡ç­¾ï¼Œç»´åº¦ä¸º(None, n_y = 6)
        X_test - æµ‹è¯•æ•°æ®ï¼Œç»´åº¦ä¸º(None, 64, 64, 3)
        Y_test - è®­ç»ƒæ•°æ®å¯¹åº”çš„æ ‡ç­¾ï¼Œç»´åº¦ä¸º(None, n_y = 6)
        learning_rate - å­¦ä¹ ç‡
        num_epochs - éå†æ•´ä¸ªæ•°æ®é›†çš„æ¬¡æ•°
        minibatch_size - æ¯ä¸ªå°æ‰¹é‡æ•°æ®å—çš„å¤§å°
        print_cost - æ˜¯å¦æ‰“å°æˆæœ¬å€¼ï¼Œæ¯éå†100æ¬¡æ•´ä¸ªæ•°æ®é›†æ‰“å°ä¸€æ¬¡
        isPlot - æ˜¯å¦ç»˜åˆ¶å›¾è°±
        
    è¿”å›ï¼š
        train_accuracy - å®æ•°ï¼Œè®­ç»ƒé›†çš„å‡†ç¡®åº¦
        test_accuracy - å®æ•°ï¼Œæµ‹è¯•é›†çš„å‡†ç¡®åº¦
        parameters - å­¦ä¹ åçš„å‚æ•°
    """
    ops.reset_default_graph()  #èƒ½å¤Ÿé‡æ–°è¿è¡Œæ¨¡å‹è€Œä¸è¦†ç›–tfå˜é‡
    tf.set_random_seed(1)    #ç¡®ä¿ä½ çš„æ•°æ®å’Œæˆ‘ä¸€æ ·
    seed = 3                 #æŒ‡å®šnumpyçš„éšæœºç§å­
    (m , n_H0, n_W0, n_C0) = X_train.shape
    n_y = Y_train.shape[1]
    costs = []
    
    #ä¸ºå½“å‰ç»´åº¦åˆ›å»ºå ä½ç¬¦
    X , Y = create_placeholders(n_H0, n_W0, n_C0, n_y)
    
    #åˆå§‹åŒ–å‚æ•°
    parameters = initialize_parameters()
    
    #å‰å‘ä¼ æ’­
    Z3 = forward_propagation(X,parameters)
    
    #è®¡ç®—æˆæœ¬
    cost = compute_cost(Z3,Y)
    
    #åå‘ä¼ æ’­ï¼Œç”±äºæ¡†æ¶å·²ç»å®ç°äº†åå‘ä¼ æ’­ï¼Œæˆ‘ä»¬åªéœ€è¦é€‰æ‹©ä¸€ä¸ªä¼˜åŒ–å™¨å°±è¡Œäº†
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
    
    #å…¨å±€åˆå§‹åŒ–æ‰€æœ‰å˜é‡
    init = tf.global_variables_initializer()
    
    #å¼€å§‹è¿è¡Œ
    with tf.Session() as sess:
        #åˆå§‹åŒ–å‚æ•°
        sess.run(init)
        #å¼€å§‹éå†æ•°æ®é›†
        for epoch in range(num_epochs):
            minibatch_cost = 0
            num_minibatches = int(m / minibatch_size) #è·å–æ•°æ®å—çš„æ•°é‡
            seed = seed + 1
            minibatches = cnn_utils.random_mini_batches(X_train,Y_train,minibatch_size,seed) 
            
            #å¯¹æ¯ä¸ªæ•°æ®å—è¿›è¡Œå¤„ç†
            for minibatch in minibatches:
                #é€‰æ‹©ä¸€ä¸ªæ•°æ®å—
                (minibatch_X,minibatch_Y) = minibatch
                #æœ€å°åŒ–è¿™ä¸ªæ•°æ®å—çš„æˆæœ¬
                _ , temp_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X, Y:minibatch_Y})
                
                #ç´¯åŠ æ•°æ®å—çš„æˆæœ¬å€¼
                minibatch_cost += temp_cost / num_minibatches
    
            #æ˜¯å¦æ‰“å°æˆæœ¬
            if print_cost:
                #æ¯5ä»£æ‰“å°ä¸€æ¬¡
                if epoch % 5 == 0:
                    print("å½“å‰æ˜¯ç¬¬ " + str(epoch) + " ä»£ï¼Œæˆæœ¬å€¼ä¸ºï¼š" + str(minibatch_cost))
            
            #è®°å½•æˆæœ¬
            if epoch % 1 == 0:
                costs.append(minibatch_cost)
        
        #æ•°æ®å¤„ç†å®Œæ¯•ï¼Œç»˜åˆ¶æˆæœ¬æ›²çº¿
        if isPlot:
            plt.plot(np.squeeze(costs))
            plt.ylabel('cost')
            plt.xlabel('iterations (per tens)')
            plt.title("Learning rate =" + str(learning_rate))
            plt.show()
        
        #å¼€å§‹é¢„æµ‹æ•°æ®
        ## è®¡ç®—å½“å‰çš„é¢„æµ‹æƒ…å†µ
        predict_op = tf.arg_max(Z3,1)
        corrent_prediction = tf.equal(predict_op , tf.arg_max(Y,1))
        
        ##è®¡ç®—å‡†ç¡®åº¦
        accuracy = tf.reduce_mean(tf.cast(corrent_prediction,"float"))
        print("corrent_prediction accuracy= " + str(accuracy))
        
        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})
        test_accuary = accuracy.eval({X: X_test, Y: Y_test})
        
        print("è®­ç»ƒé›†å‡†ç¡®åº¦ï¼š" + str(train_accuracy))
        print("æµ‹è¯•é›†å‡†ç¡®åº¦ï¼š" + str(test_accuary))
        
        return (train_accuracy,test_accuary,parameters)
```

**ç»“æœ**
![](21.png)  
![](22.png)

